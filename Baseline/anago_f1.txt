Using all hyperparam no embedding

>>> model.eval(x_test, y_test)
 - f1: 66.70

Using all hyperparam with glove embedding

>>> model.eval(x_test, y_test)
 - f1: 71.52

>>> printConfMat()
	B-LOC 	B-MISC 	B-ORG 	B-PER 	I-LOC 	I-MISC 	I-ORG 	I-PER 	O
B-LOC 	163 	21 	16 	2 	3 	2 	3 	1 	7 	
B-MISC 	17 	59 	26 	5 	1 	10 	4 	0 	19 	
B-ORG 	10 	20 	227 	9 	6 	4 	7 	1 	12 	
B-PER 	7 	9 	9 	370 	1 	7 	7 	21 	7 	
I-LOC 	4 	1 	0 	0 	115 	7 	8 	0 	6 	
I-MISC 	2 	11 	3 	3 	15 	63 	25 	7 	25 	
I-ORG 	2 	2 	4 	3 	12 	10 	92 	3 	23 	
I-PER 	0 	0 	0 	7 	3 	6 	4 	192 	2 	
O 	11 	29 	32 	35 	0 	12 	9 	5 	7185 	


>>> print classification_report(y_true, y_pred)
             precision    recall  f1-score   support

      B-LOC       0.75      0.75      0.75       218
     B-MISC       0.39      0.42      0.40       141
      B-ORG       0.72      0.77      0.74       296
      B-PER       0.85      0.84      0.85       438
      I-LOC       0.74      0.82      0.77       141
     I-MISC       0.52      0.41      0.46       154
      I-ORG       0.58      0.61      0.59       151
      I-PER       0.83      0.90      0.86       214
          O       0.99      0.98      0.98      7318

avg / total       0.93      0.93      0.93      9071

>>> f1_score(y_true, y_pred, average='micro', labels=labels)
0.72413793103448276

---------------------------------------------------------------------------------------------

>>> model.eval(x_test, y_test)
 - f1: 64.38

             precision    recall  f1-score   support

      B-LOC       0.72      0.70      0.71       218
     B-MISC       0.32      0.33      0.32       141
      B-ORG       0.70      0.78      0.74       296
      B-PER       0.91      0.61      0.73       438
      I-LOC       0.74      0.79      0.77       141
     I-MISC       0.32      0.36      0.34       154
      I-ORG       0.54      0.64      0.59       151
      I-PER       0.91      0.65      0.76       214
          O       0.97      0.98      0.98      7318

avg / total       0.92      0.91      0.91      9071

>>> from sklearn.metrics import confusion_matrix
>>> print confusion_matrix(y_true, y_pred)
[[ 153   17   19    2    3    9    3    0   12]
 [  19   46   32    2    0   10    5    1   26]
 [   8   13  231    4    7    5    7    0   21]
 [   4   21   12  266    0   19    8    8  100]
 [   2    2    1    0  111   11   12    0    2]
 [   4    6    5    1   14   56   28    4   36]
 [   1    0    1    1   11   16   97    1   23]
 [   0    0    0    9    2   32   11  139   21]
 [  22   38   27    8    1   15    9    0 7198]]
>>> 
>>> print f1_score(y_true, y_pred, average='micro', labels=labels)
0.649335302806
>>> print f1_score(y_true, y_pred, average='macro', labels=labels)
0.619212005878

-----------------------------------------------------------------------------------

 - f1: 67.06
             precision    recall  f1-score   support

      B-LOC       0.75      0.70      0.72       218
     B-MISC       0.46      0.25      0.32       141
      B-ORG       0.79      0.69      0.73       296
      B-PER       0.79      0.74      0.77       438
      I-LOC       0.74      0.77      0.75       141
     I-MISC       0.46      0.34      0.39       154
      I-ORG       0.62      0.54      0.58       151
      I-PER       0.75      0.83      0.79       214
          O       0.96      0.99      0.98      7318

avg / total       0.91      0.92      0.92      9071

[[ 153   10    8   13    2    1    1    2   28]
 [  19   35   14    8    0   10    3    1   51]
 [  10    7  203   24    5    0    9    1   37]
 [   2    2    1  324    1   13    5   20   70]
 [   2    1    1    0  109   10    5    8    5]
 [   5    5    3    5   15   52   16   13   40]
 [   5    0    3    3   13    6   81   10   30]
 [   0    0    0   10    2   12    2  178   10]
 [   9   16   24   21    1   10    8    3 7226]]
0.632016011264
0.68229636309

-------------------------------------------------------------------------


